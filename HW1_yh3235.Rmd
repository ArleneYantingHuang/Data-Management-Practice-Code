---
title: "APAN5335 Homework 1"
output:
  word_document: default
  pdf_document: default
---


For this assignment, the following libraries are required, please install and load them by running the following:

```{r, message=FALSE, warning=FALSE}

required_packages = c( "tidyverse", "corrplot", "gridExtra",
                       "GGally", "cluster", "factoextra", 
                       "mlbench", "class", "ggplot2", "FNN",
                       "caret", "plot3D", "Rtsne")
packages_to_install = setdiff(required_packages,
                              installed.packages()[,"Package"])

if (length(packages_to_install)!=0) {
  install.packages(packages_to_install)
}

library(mlbench)
library(ggplot2)
library(class)
library(FNN)
library(caret)
library(dplyr)
library(tidyr)
library(corrplot)
library(cluster)
library(plot3D)
library(Rtsne)

# let's set the seed to 0 for consistency
set.seed(0)

#clear enviroment
rm(list = ls())

```


## Question 1: Data Visualization (10 pts)

For question 1, we'll be working with the wine dataset from UCI Machine Learning Repository. The data can be downloaded directly from the repository (see code below).

### 1.1 Read in the data and report its dimension. (2 pts)

```{r cars}
# read in the data using the following
wine = read.csv(
  url("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"),
  header = F, 
  col.names = c("Wine", "Alcohol", "Malic.acid", "Ash", 
                "Acl", "Mg", "Phenols", "Flavanoid",
                "Nonflavanoid.phenols", "Proanth", "Color.int", 
                "Hue", "OD", "Proline"))

# display the first few columns using head()
head(wine)

# report dimension
ncol(wine)

```

### 1.2 Show the summary statistics of the dataset. (2 pts)

```{r}
# show summary statistics of the data
str(wine)

```

### 1.3 What is the pairwise relationship of the first five variables? Use the pairs function to display scatterplots of each pair of these variables. (2 pts)

```{r}
# pair plot of wine
# method1
pairs(wine[1:5],
      main = 'Pairwise Relationship of First 5 Variables -- Wine Dataset')

```

### 1.4 Plot a histogram of each attribute. (2 pts)

```{r}

# histogram plot of each attribute
for (i in 1:14) {
  hist(wine[,i], main = paste0('Histogram of ',colnames(wine)[i]), xlab = colnames(wine)[i])
}

```

### 1.5 To understand the relationship between each pair of attributes, construct a correlation matrix using corrplot(). (2 pts)

```{r}
# correlation plot
library(corrplot)
corr = cor(wine)
corrplot(corr, method = "square",
         order = "AOE",
         addCoef.col = "black",
         tl.col = "black",
         tl.cex = 0.5,
         number.cex=0.4)
```


## Question 2. Clustering (10 pts)

### 2.1 Read in the iris data set. Subset your data to only include "Sepal Length", "Sepal Width", "Petal Length", and "Petal Width". Rescale your data such that the values are between 0 and 1. Appply K-means clustering with the number of clusters as 3. Compare your clusters with the actual class using table(). (3 pts)

```{r}

rm(list = ls())
# read in "iris" using data()
library(datasets)
data(iris)

# split your data in to a features ("Sepal Length", "Sepal Width", "Petal Length", and "Petal Width") and target ("Species)
features = iris[,1:4]
target = data.frame(iris[,5])

# normalize your features
# use z-score method
features = data.frame(scale(features))

# fit a k-means model with k = 3
km <- kmeans(features, centers=3)

# compare your cluster results with the target variable using table()
result = data.frame(target,km$cluster)
table(result)
```


### 2.2 Draw a 2D clustering plot using clustplot(). (2 pts)

```{r}
# plot a 2D clustering plot using clustplot()
library(cluster)
clusplot(pam(features,3), main = "2D clustering plot for iris dataset")
```

### 2.3 Draw a 3D plot using scatter3D. (2 pts)

```{r}
# draw a 3D plot using scatter3D
library(plot3D)
scatter3D(x = features$Sepal.Length, 
          y = features$Sepal.Width, 
          z = features$Petal.Length, 
          theta = 20, phi = 20, 
          pch = 16, cex = 0.5, 
          col.var = as.integer(iris$Species),  #use data from iris as cluster data
          col = c("#009999", "#D95F02", "#660000"), 
          labels = c("setosa", "versicolor", "virginica"), 
          colkey = list(at = c(2, 3, 4), side = 1,
                        length = 0.4, width = 0.5,
                        labels = c("setosa", "versicolor", "virginica")))
```

### 2.4 Visualize high-dimensional (>3 dimensions) data by first reducing the number of dimensions to 2 using t-SNE. Plot your t-SNE dimensions colored by the class labels. (3 pts)

```{r}
# transform your features data into a matrix
features_matrix = as.matrix(features)

# use Rtsne() to reduce your dimensions, set perplexity = 20, theta = 0.5, dims = 2
library(Rtsne)
set.seed(1213) #set a seed in case the number changes
features_tsne = Rtsne(features_matrix, perplexity = 20, theta = 0.5, dims = 2, check_duplicates = F) #you can either  set check_duplicates to false or  remove duplicate value, I choose to set the argument to false here

# display the results of t-SNE colored by the cluster labels
library(ggplot2)
ggplot(data.frame(features_tsne$Y), aes(x = X1, y = X2, color = iris$Species)) +
  geom_point() +
  scale_color_discrete(name = "Species") +
  labs(x = "Component1", y ="Component2", title = "t-SNE Result")
  
```


## Question 3. kNN Classification (10 pts)


### 3.1 Read in "PimaIndiansDiabetes" dataset using "data(PimaIndiansDiabetes)". Describe the features in the data using summary(). Plot a histogram of the variable "age" and describe its distribution. Compute the proportion of individuals with diabetes. (2 pt)

```{r}

# read in the data, store the data as "df"
rm(list = ls())
data(PimaIndiansDiabetes)
df = data.frame(PimaIndiansDiabetes)
rm(PimaIndiansDiabetes)

# describe the data using summary()
summary(df)

# plot histogram of age
hist(df$age, main = "Histogram of Age - PimaIndiansDiabetes Dataset")

# proportion of individuals with diabetes
prop.table(table(df$diabetes)) #Method1: show prop table, positive = 34.9%
table(df$diabetes)[2]/(table(df$diabetes)[1]+table(df$diabetes)[2]) #Method2: directly show the proportion

```

The distribution of "age" variable is skewed with more younger individuals than older.

### 3.2 Perform a scatter plot of glucose and mass, colored by diabetes. (2 pt)

```{r}
# convert "pos" in diabetes variable to factor
df[df$diabetes == "pos","diabetes"] = as.factor(df[df$diabetes == "pos","diabetes"]) #as the question required
#or
df$diabetes = as.factor(df$diabetes)

# plot glucose vs mass, colored by diabetes
library(ggplot2)
ggplot(df, aes(x = glucose, y = mass, color = diabetes)) +
  geom_point()

```

### 3.3 Use "knn3" command to build a k-NN classifier, setting k to 2, 50, and 200. Visualize the decision boundaries using the "decision_boundary_plot" function below. (3 pts)

```{r}

# function to plot decision boundary
decision_boundary_plot <- function(model, data, x1_var, x2_var, y_var, resolution = 100, title) {
  
  x1 = data[, x1_var]
  x2 = data[, x2_var]
  y = data[, y_var]
  
  # make grid
  xs1 <- seq(min(x1), max(x1), length.out = resolution)
  xs2 <- seq(min(x2), max(x2), length.out = resolution)
  g <- cbind(rep(xs1, each=resolution), rep(xs2, time = resolution))
  g <- as.data.frame(g)
  colnames(g) = c(x1_var, x2_var)
  
  p <- predict(model, g, type = "class")
  
  plt = ggplot() + 
    geom_point(aes(g[, x1_var], g[, x2_var], col = p), size = 0.1) +
    geom_point(aes(x1, x2, col = y), size = 2) + 
    geom_contour(
      aes(x = g[, x1_var], y = g[, x2_var],z= as.integer(p)), 
      col = 'black', size = 0.1) +
    xlab(x1_var) + ylab(x2_var) + scale_colour_discrete(name=y_var) +
    ggtitle(title)
  
  return(plt)
}
```

```{r}
# build a k-NN classifier, setting k to 2
knn_2 = knn3(diabetes~glucose+mass, df, k = 2) #still use glucose and mass as  previous question

# visualize the decision boundary of k-NN, k = 2
decision_boundary_plot(knn_2, df, "glucose", "mass", "diabetes", title = "k = 2 decision boundary plot")

# build a k-NN classifier, setting k to 50
knn_50 = knn3(diabetes~glucose+mass, df, k = 50) #still use glucose and mass as  previous question

# visualize the decision boundary of k-NN, k = 50
decision_boundary_plot(knn_50, df, "glucose", "mass", "diabetes", title = "k = 50 decision boundary plot")

# build a k-NN classifier, setting k to 200
knn_200 = knn3(diabetes~glucose+mass, df, k = 200) #still use glucose and mass as  previous question

# visualize the decision boundary of k-NN, k = 200
decision_boundary_plot(knn_200, df, "glucose", "mass", "diabetes", title = "k = 200 decision boundary plot")

```

### 3.4 What happens to the decision boundary for k-NN as k increases? Intuitively, why does this happen? From the visualizations, which k is optimal? (3 pts)

When k increases, the decision boundary line becomes smoother and less leaning. That is because as k increases, there are more points engaged in deciding the color of an area in the plot, which makes the decision less biased. However, excessively high k may make the decision boundary over smooth and unable to show the feature of the data. 
Personally, I would say that k = 50 is optimal form the visualizations, as it is much smoother than the k = 2 plot (there is a lot of red and blue holes in the k = 2 plot, the decision boundary line is very unclear) and shows more features than the k = 200 plot (as there are only 768 data points, k = 200 was too much).


## Question 4. Regression (10 pts)

### 4.1 Read in "airquality" dataset using "data(airquality)". Describe the data using summary(). Use pair() to observe the pairwise relationships between the variables. Do most of these pairwise relationships seem linear or nonlinear? (2 pts)

```{r}

# read in "airquality"
rm(list = ls())
data(airquality)

# store the data as df
df = data.frame(airquality)
rm(airquality)

# describe the data using summary()
summary(df)

# observe the pairwise relationships between the variables 
pairs(df, main = "Pairwise Relationship -- Airquality")

```

From the pair plot, most of the relationships appear to be nonlinear.


### 4.2 Regress Temp (y) on Ozone (x) using linear regression and subsequently a third-order polynomial regression. Perform a scatter plot of "Ozone" and "Temp" with the fitted linear and polynomial regressions. Which is better? (2 pts) 

```{r}

# read in airquality
# done from 4.1

# subset the data to include only "Ozone" and "Temp". Remove missing values.
df = df[!is.na(df$Ozone),]
df = df[!is.na(df$Temp),]
df = df[,c("Ozone","Temp")]

# regress Temp (y) on Ozone (x) using linear regression
model = lm(Temp~Ozone, df)
summary(model)

# regression Temp (y) on Ozone (x) using 3rd-degree polynomial regression
model_poly = lm(Temp~Ozone+I(Ozone^2)+I(Ozone^3), df)
summary(model_poly)

# plot Temp vs Ozone with the fitted regression line
plot(df$Ozone, df$Temp, main = "Linear & Polynomial Regression on Temp-Ozone From Airqulity Dataset", xlab = "Ozone", ylab = "Temp")
abline(model, lwd = 3, col = "red")
lines(smooth.spline(df$Ozone, predict(model_poly)), col = "blue", lwd = 3)


```

Visually, polynomial regression fits the data better.


### 4.3 Use "knn.reg" from "FNN" library to perform a k-NN regression to predict "Temp" given "Ozone". Set k to 2, 10, and 100. Perform a scatter plot of Ozone and Temp with the kNN predictions. (3 pts)

```{r}
# fit a kNN regression with k = 2
library(FNN)
knn_reg_2 = knn.reg(df$Ozone, y = df$Temp, k = 2)

# plot Temp vs Ozone with kNN predictions, k = 2
plot(df$Ozone, knn_reg_2$pred, main = "KNN Regression Prediction (K = 2)", xlab = "Ozone", ylab = "Predicted Temp")

# fit a kNN regression with k = 10
knn_reg_10 = knn.reg(df$Ozone, y = df$Temp, k = 10)

# plot Temp vs Ozone with kNN predictions, k = 10
plot(df$Ozone, knn_reg_10$pred, main = "KNN Regression Prediction (K = 10)", xlab = "Ozone", ylab = "Predicted Temp")

# fit a kNN regression with k = 100
knn_reg_100 = knn.reg(df$Ozone, y = df$Temp, k = 100)

# plot Temp vs Ozone with kNN predictions, k = 100
plot(df$Ozone, knn_reg_100$pred, main = "KNN Regression Prediction (K = 100)", xlab = "Ozone", ylab = "Predicted Temp")


# compare the results
knn_reg_2_result = data.frame(Ozone = df$Ozone, result = knn_reg_2$pred, k = 2)
knn_reg_10_result = data.frame(Ozone = df$Ozone, result = knn_reg_10$pred, k = 10)
knn_reg_100_result = data.frame(Ozone = df$Ozone, result = knn_reg_100$pred, k = 100)
result = rbind(knn_reg_2_result, knn_reg_10_result, knn_reg_100_result)
rm(knn_reg_2_result, knn_reg_10_result, knn_reg_100_result)
library(ggplot2)
ggplot(result, aes(x = Ozone, y = result, color = as.factor(k))) +
  geom_point() +
  scale_color_discrete(name = "K =") +
  labs(x = "Ozone", y ="Predicted Temp", title = "KNN Regression Prediction Comparison")

```


### 4.4 Describe the predictions as k increases. Intuitively, why does this happen? Is this behavior similar in a k-NN classification? Which k is optimal? Compare kNN regression with linear regression. (3 pts)

As k increases, the regression prediction line becomes smoother and more horizontal; and as k increases, the predicted temperature tends to converge around Temp = 78. This is similar to what happend in kNN classification.

Given that the model of k = 10 have highest R-square, I would say k = 10 is optimal among these three models. (If more values of k is explored, there may be a different opitimal k. Also, if taking other factors into consideration, there may be a diffirent answer, but for now I will simply use R-square.)

Compared with linear model, both k = 2 and k = 10 model have higher R-square than linear model, indicating they have better fits than linear model. As the plot also shows that Ozone and Temp have non-linear relationship, I think k = 10 model is the best model among all four models.


```{r}
knn_reg_2$R2Pred
knn_reg_10$R2Pred
knn_reg_100$R2Pred
summary(model)$r.squared
rm(list = ls())
```