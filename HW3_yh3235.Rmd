---
title: "HW3"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---


```{r, message=FALSE, warning=FALSE}

required_packages = c("mlbench", "ggplot2", "e1071", "ROCR")
packages_to_install = setdiff(required_packages, installed.packages()[,"Package"])

if (length(packages_to_install)!=0) {
  install.packages(packages_to_install)
}

library(mlbench)
library(ggplot2)
library(e1071)
library(tidyverse)  
library(modelr)    
library(broom)      
library(ROCR)


set.seed(0)

```



## Question 1. Support Vector Machine.In this question, we'll implement SVM and visualize its decision boundaries as we modify its parameters. We'll reuse "PimaIndiansDiabetes" dataset and the decision boundary plot from homework 1. (20 pts)

### 1.1 Data Loading & Processing (3 pts)

```{r}
rm(list = ls())
# load in PimaIndiansDiabetes
data(PimaIndiansDiabetes)

# binarize "pos" (1 = 'pos', 0 ='neg') in diabetes variable
PimaIndiansDiabetes$diabetes = as.factor(ifelse(PimaIndiansDiabetes$diabetes == 'pos', 1,0))

# scatter plot glucose vs mass, coloring the points by diabetes
library(ggplot2)
ggplot(data = PimaIndiansDiabetes, aes(x = glucose, y = mass))+
  geom_point(aes(color = as.factor(diabetes)))

```


```{r}

# split data into training & testing (70/30 split)
library(caret)
# set.seed(1213)
split = createDataPartition(y = PimaIndiansDiabetes$diabetes, p = 0.7, list = FALSE)
train = PimaIndiansDiabetes[split,]
test = PimaIndiansDiabetes[-split,]

```


### 1.2 SVM & Varying Degree of Polynomial Kernel.Let's fit SVM to our data and visualize the decision boundaries as we change the parameters. Please use the following formula when fitting your model: diabetes ~ glucose + mass. (5 pts)


```{r}

# function to plot decision boundary
decision_boundary_plot <- function(model, data, x1_var, x2_var, y_var, resolution = 100, title) {
  
  x1 = data[, x1_var]
  x2 = data[, x2_var]
  y = data[, y_var]
  
  # make grid
  xs1 <- seq(min(x1), max(x1), length.out = resolution)
  xs2 <- seq(min(x2), max(x2), length.out = resolution)
  g <- cbind(rep(xs1, each=resolution), rep(xs2, time = resolution))
  g <- as.data.frame(g)
  colnames(g) = c(x1_var, x2_var)
  
  p <- predict(model, g, type = "class")
  
  plt = ggplot() + 
    geom_point(aes(g[, x1_var], g[, x2_var], col = p), size = 0.1) +
    geom_point(aes(x1, x2, col = y), size = 2) + 
    geom_contour(
      aes(x = g[, x1_var], y = g[, x2_var],z= as.integer(p)), 
      col = 'black', size = 0.1) +
    xlab(x1_var) + ylab(x2_var) + scale_colour_discrete(name=y_var) +
    ggtitle(title)
  
  return(plt)
}


```  

Let's use polynomial kernel and vary its degree from 1-3, the observe our decision boundaries.

```{r,fig.align='center'}

# fit using "svm", with "polynomial" kernel, degree = 1, cost = 5, type = "C", and scale = F
library(e1071)
svm1 = svm(diabetes ~ glucose + mass, train, kernel = "polynomial", degree = 1, scale = F, type = "C", cost = 5)

# visualize the decision boundary of your SVM model
decision_boundary_plot(svm1, train, x1_var = 'glucose', x2_var = 'mass', y_var = 'diabetes', title = 'polyminomial svm model 1 (degree = 1)')

# assess the accuracy of your prediction on the testing set
y_test_pred = predict(svm1, newdata = test[,-9])
cm_test = table(test[,9], y_test_pred); cm_test
accuracy = sum(cm_test[1,1], cm_test[2,2])/length(test$diabetes)
print(paste0("accuracy = ", accuracy))

```


```{r}
# fit using "svm", with "polynomial" kernel, degree = 2, cost = 5, type = "C", and scale = F
svm2 = svm(diabetes ~ glucose + mass, train, kernel = "polynomial", degree = 2, scale = F, type = "C", cost = 5)

# visualize the decision boundary of your SVM model
decision_boundary_plot(svm2, train, x1_var = 'glucose', x2_var = 'mass', y_var = 'diabetes', title = 'polyminomial svm model 2 (degree = 2)')

# assess the accuracy of your prediction on the testing set
y_test_pred = predict(svm2, newdata = test[,-9])
cm_test = table(test[,9], y_test_pred); cm_test
accuracy = sum(cm_test[1,1], cm_test[2,2])/length(test$diabetes)
print(paste0("accuracy = ", accuracy))

```

```{r}
# fit using "svm", with "polynomial" kernel, degree = 3, cost = 5, type = "C", and scale = F
svm3 = svm(diabetes ~ glucose + mass, train, kernel = "polynomial", degree = 3, scale = F, type = "C", cost = 5)

# visualize the decision boundary of your SVM model
decision_boundary_plot(svm3, train, x1_var = 'glucose', x2_var = 'mass', y_var = 'diabetes', title = 'polyminomial svm model 3 (degree = 3)')

# assess the accuracy of your prediction on the testing set
y_test_pred = predict(svm3, newdata = test[,-9])
cm_test = table(test[,9], y_test_pred); cm_test
accuracy = sum(cm_test[1,1], cm_test[2,2])/length(test$diabetes)
print(paste0("accuracy = ", accuracy))

```


### 1.3 What happens to our decision boundary as the degree of the polynomial kernel increases? (2 pts)
The decision boundary function started from X to X^2 to X^3 and become more curved and distorted. Meanwhile, the accuracy of models is declining. It seems that the data works better for linear boundary.


### 1.4 SVM & Varying Cost. Let's vary our "cost" parameter and observe our decision boundaries.(2 pts)

```{r}
# fit using "svm", with "polynomial" kernel, degree = 3, cost = 50, type = "C", and scale = F
svm4 = svm(diabetes ~ glucose + mass, train, kernel = "polynomial", degree = 3, scale = F, type = "C", cost = 50)

# visualize the decision boundary of your SVM model
decision_boundary_plot(svm4, train, x1_var = 'glucose', x2_var = 'mass', y_var = 'diabetes', title = 'polyminomial svm model 4 (cost = 50)')

# assess the accuracy of your prediction on the testing set
y_test_pred = predict(svm4, newdata = test[,-9])
cm_test = table(test[,9], y_test_pred); cm_test
accuracy = sum(cm_test[1,1], cm_test[2,2])/length(test$diabetes)
print(paste0("accuracy = ", accuracy))

```


### 1.5 What happens to our decision boundary as we increase "cost"? (3 pts)
The cost parameter decides how much an SVM should be allowed to “bend” with the data. For a low cost, you aim for a smooth decision surface and for a higher cost, you aim to classify more points correctly. When cost increases, our decision boundary is allowed to “bend” with the data more, which increases accuracy while making the decision boundary more curved.


### 1.6 SVM Parameter Tuning. Let's fit all of the data using SVM and search for the optimal parameter using 'tune.svm'. Vary your cost parameter as c(4, 8, 16, 32), set your kernel to "linear", type = 'C', and scale = F. Using formula: diabetes ~ glucose + mass.(5 pts)

```{r}
# tune your SVM model
result = tune.svm(diabetes ~ glucose + mass, data = train, cost = c(4, 8, 16, 32), kernel = "linear", type = "C", scale = F)

# report the accuracy of your best performing model
result$best.parameter$cost
svm5 = svm(diabetes ~ glucose + mass, data = train, cost = 32, kernel = "linear", type = "C", scale = F)
y_test_pred = predict(svm5, newdata = test[,-9])
cm_test = table(test[,9], y_test_pred); cm_test
accuracy = sum(cm_test[1,1], cm_test[2,2])/length(test$diabetes)
print(paste0("accuracy = ", accuracy))
```


```{r}
# visualize the decision boundary of your SVM model
decision_boundary_plot(svm5, train, x1_var = 'glucose', x2_var = 'mass', y_var = 'diabetes', title = 'polyminomial svm model 5 (best model of cost)')
```

## Question 2.Logistic Regression. In this exercise, you will work with logistic regression and we are using default data provided by ISLR package. This simulated dataset contains information on ten thousand customer such as whether the customer defaulted, is a student, the average balance carried by the customer and the income of the customer.(20 pts)

### 2.1 Load the dataset and preview it (2 pts)
```{r}
# load dataset
rm(list = ls())
library(ISLR)
data(Default)
head(Default)
str(Default)

```
### 2.2 split your data into traing and testing datasets (70% and 30%). Use seed(123) to be consistent (2 pts)
```{r}
# split data into training & testing (70/30 split)
library(caret)
set.seed(123)
split = createDataPartition(y = Default$default, p = 0.7, list = FALSE)
train = Default[split,]
test = Default[-split,]
```
### 2.3 Fit a logisitic regression model to predict the probability of a customer defaultig based on the average balance carried by the customer (4 pts)
```{r}
#use glm function 
library(stats)
logistic1 = glm(default ~ balance, data = train, family = "binomial")

```

### 2.4 Plot the logistic function (4 pts)
```{r}
#plot logistic function
newdata = data.frame(balance = seq(min(Default$balance), max(Default$balance), length.out = 1500))
newdata$pred = predict(logistic1,newdata=newdata,type='response') 

ggplot(newdata, aes(x = balance, y = pred))+
  geom_line()

```

### 2.5 predict the probability of defualting based on balance of $1000 and $2000 (2 pts)
```{r}
#predict the probability of defualting
pred = predict(logistic1, data.frame(balance = c(1000,2000)), type="response")
pred = data.frame(pred)
rownames(pred) = c("$1000", "$2000")
pred
```
### 2.6 Test the predicted target varaiable vs. the observed values of the model( 2 pts)
```{r}
#Test the predicted target varaiable vs. the observed values
pred = predict(logistic1, newdata = test, type='response')
pred = as.integer(pred>0.5)
data.frame(pred, default = (as.integer(test$default)-1))
```
### 2.7 Form the confusion matrix to see the classification performance (2 pts)
```{r}
#Form the confusion matrix
ct = table(pred, default = (as.integer(test$default)-1)); ct
```
### 2.8 Determine the misclassification rate(2 pts)
```{r}
#misclassification rate
accuracy = sum(ct[1,2],ct[2,1])/nrow(test)
print(paste0("accuracy = ", accuracy))

```